---
layout: post
title: cs231n Lec6
subtitle: Training Neural Networks I
categories: cs231n
tags: CV cs231n
sidebar: []
---





[cs231n 강의](https://youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)

<br>

**Activation Functions**

![image](https://user-images.githubusercontent.com/71377968/164890178-171bbf89-30d5-4cfa-94ec-f33ac9b7ff0c.png)

Sigmoid: 0과 1사이의 output을 내놓음. 문제점은 아래와 같음.

- saturation되는 것이 gradient를 kill할 수 있음.
- zero-centered되지 않는 output
- exp() 함수가 비용이 많이 듦. (이 문제는 그리 크지 않음)

-> input이 모두 같은 부호가 되면, W가 모두 같은 방향으로만 움직이게 됨. 그래서 파라미터 업데이트 시에 다같이 증가하거나 다같이 감소하여 사분면 중 두개의 영역만 사용하게 됨. 따라서 zero-mean data인 것이 좋음

<br>

tanh: sigmoid의 zero-centered 문제를 해결한 모습으로, -1과 1사이의 output을 내놓음.

- 여전히 saturation 문제 있음

<br>

ReLU: 0보다 작은 input에서는 0을 output으로, 그 외의 구간에서는 x를 output으로 내놓음.

- 일부 saturation 문제 해결
- 계산 효율 좋음
- zero-centered data가 아님
- 양수값에서는 saturation 발생하지 않으나 음수값에서는 saturation 발생

-> dead ReLU: data의 절반만 activate될 수 있음.

- 초기화를 잘못한 경우
- learning rate가 지나치게 높은 경우

<br>

Leaky ReLU: 음의 구간에 어떤 파라미터 alpha를 곱해 음의 구간에서의 saturation 문제 해결

ELU: exp() 이용한 것. 하지만 saturation될 수 있음

Maxout Neuron: 기존 파라미터의 두배의 파라미터 수를 가지게 됨. ReLU와 Leaky ReLU를 좀 더 일반화 시킨 것.

![image](https://user-images.githubusercontent.com/71377968/164890180-2bc64907-e759-45dd-a9e9-cb7e1c7a627c.png)

결론적으로, ReLU를 주로 사용하고 Leaky ReLU, Maxout, ELU를 시도해볼 수 있음. tanh도 가능은 하나 권장하지 않음. 그러나 sigmoid는 사용하지 않는 것이 좋음

<br>

**Data Preprocessing**

![image](https://user-images.githubusercontent.com/71377968/164890187-4ab44109-fde9-4d60-b2b3-ac91debaf5c1.png)

주로 zero-centered data로 만들어주고, normalization을 거침. normalization은 모든 차원이 동일한 범위 내에 존재하게 하여 동등한 기여를 할 수 있게 함. (주로, 이미지의 경우는 zero-centered data까지만 맞춰줌. 이미지는 대부분 이미 스케일이 어느정도 맞춰져있어서 그 외의 것들은 굳이 필요 없음.)

<br>

**Weight initialization**

모든 W=0이라면, 모든 뉴런이 전부 똑같은 연산을 수행하게 될 것. 모두 같은 연산을 하면, output이 모두 같아지고, gradient도 같아짐. 모든 가중치가 똑같은 값으로 업데이트 되며, 모든 뉴런의 모양이 같아질 것.(=> Symmetric breaking") 따라서, weight는 같은 값으로 초기화해서는 안됨.

해결)

- weight를 임의의 작은 값으로 초기화: 작은 network에서는 괜찮지만 좀 더 깊은 network에서는 update가 제대로 이루어지지 않을 것.

- weight를 그보다 큰 값으로 초기화: saturation 발생으로 출력은 항상 +1/-1의 값만 가지며, 업데이트가 제대로 이루어지지 않을 것.

- > : 입출력의 분산을 막아줌. 그러나 ReLU를 사용하게 되면 잘 작동하지 않음 =>ReLU는 데이터의 절반을 0으로 만들어버리므로, 분포가 줄어들게 됨. 이를 해결하기 위해 뉴런의 절반이 사라졌다는 것을 반영하여 2로 나눠주면 됨.

  Xavier initialization

  <br>

**Batch Normalization**

Gradient Vanishing이 나오지 않도록 하는 아이디어. activation function의 변화가 아닌 training 과정 자체를 안정화하는 역할을 함.

- training하는 동안 모든 layer의 input이 unit gaussian이 됐으면 함 -> forward pass 동안 그렇게 되도록 명시적으로 만들어 주면 됨. -> batch 단위로 한 레이어에 input으로 들어오는 모든 값들을 이용하여 평균과 분산을 구해 normalization 해주면 됨. (미분 가능한 함수에서 가능함; 평균과 분산을 상수로 가지고만 있으면 미분 가능)

![image](https://user-images.githubusercontent.com/71377968/164890190-6ff35ac9-e69e-4e4f-b720-9c89a8aee576.png)

BN은 input의 scale만 살짝 변경하는 역할을 하며, FC나 Conv 뒤에 사용함. bad scaling effect가 발생하나, 해당 bad effect는 충분히 상쇄될 수 있음.

![image](https://user-images.githubusercontent.com/71377968/164890193-88350f52-760c-4674-bcb8-b0d6e378e44f.png)

- 모든 mini-batch마다 각각 평균과 분산을 계산해주고, normalization 이후 scaling, shifting factor를 사용함
- gradient의 흐름을 보다 원활하게 해주며, 학습이 더 잘되게 해주는 역할
- learning rate를 높일 수 있으며, 다양한 initialization을 수행할 수 있음
- regularization의 역할을 수행하기도 함
- 선형 변환으로, 공간적 구조가 잘 유지됨
- test phase에서 새로운 계산을 굳이 요구하지 않음

<br>

**Babysitting the Learning Process**

1. Preprocess the data: zero-center, normalization (cv에서는 굳이 normalization x)

2. Choose the architecture: hidden layer를 어떻게 구성할 것인지 대략적으로 선택

3. Check the loss is reasonable: sanity check로 layer가 동작하는걸 확인함

4. Train: 작은 dataset을 먼저 넣어보고, 이때 overfitting되어 train accuracy가 100%가 나오면 모델이 동작하고 있음을 의미함

5. Training with regularization and learning rate: 각각에 대해 적잘한 값을 찾아봄. 이때 cost값과 training값 이용

   <br>

   <br>

**Hyperparameter Optimization**

Cross-validation: training set으로 학습시키고, validation set으로 평가하는 방법. 값을 넓은 범위(coarse stage)에서 좁은 범위(fine stage)로 줄여나가는 것. learning rate는 gradient와 곱해지므로, 선택 범위는 log scale(-> 차수만 사용)을 사용하는 것이 좋음.

Grid search: 하이퍼파라미터를 고정된 값과 간격으로 샘플링하는 것. 정해진만큼의 sampling만 가능하므로 good region을 찾지 못할 수 있음

<br>

Random search: 랜덤값을 이용하는 것. grid search보다 좋음. important variable에서 더 다양한 값을 샘플링할 수 있음

![image](https://user-images.githubusercontent.com/71377968/164890198-2154c950-c6b0-4952-b2b9-f04bd73981c3.png)

+) 하이퍼파라미터 종류: learning rate, decay schedule, update type, regularization, network architecture, hidden unit과 depth의 수 등

<br>

![image](https://user-images.githubusercontent.com/71377968/164890201-562c85bf-9063-4198-a3de-f4f8ce1a5592.png)

- loss가 발산하면 learning rate이 높은 경우고, 평평하다면 learning rate이 너무 낮은 경우임. loss가 평평하다가 갑자기 가파르게 내려가는 경우는 주로 초기화에서 문제가 발생했다고 볼 수 있음(gradient의 backprop이 잘 되지 않다가 학습이 진행되면서 회복되는 것). 위의 좌측이 최적의 learning rate의 모습이라 할 수 있음

  <br>

![image](https://user-images.githubusercontent.com/71377968/164890202-f57fef7b-b16e-4c25-a25f-a5f01daec896.png)

- train_acc와 va_cc의 차이가 크다면, overfit일 수 있음 -> regularization의 강도 높이기
- gap이 없는 경우, overfit이 아직 일어나지 않은 경우로, capacity를 높일 수 있는 여유가 된다는 의미임
