---
layout: post
title: cs231n Lec4
subtitle: Introduction to Neural Networks
categories: cs231n
tags: CV cs231n
sidebar: []
---





[cs231n 강의](https://youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)



이번에는 cs231n lecture 4를 학습하고 정리를 해보았다.

### Backpropagation: computational graph를 통해 이용 가능함

- parameter update를 위해서는 gradient를 구해야하는데, 복잡한 모델에서는 이를 구하기가 쉽지 않음. 이 때 이용하는 것이 backpropagation
- gradient를 얻기 위해 computational graph 내부의 모든 변수에 대해 chain rule을 재귀적으로 사용
- 복잡한 함수를 이용하여 작업할 때 유용하게 쓰임

![image](https://user-images.githubusercontent.com/71377968/164460097-7e25d207-5bf7-418d-bed6-4e909ff42bea.png)

- computational grah를 이용하면 backpropagation 진행을 더 빠르게 할 수 있고, 미분도 빠르게 할 수 있음
- 각 노드는 연산 단계를 나타냄

아래는 예제

![image](https://user-images.githubusercontent.com/71377968/164460161-f5580c4c-aa76-42a0-9f8b-451bbcc184fc.png)

- forward pass: 순차적으로 함수값을 구하는 것
- backward pass: 역방향으로 차례로 미분하여 gradient를 구하는 것 (backpropagation)
- ∂f/∂f, ∂f/∂z, ∂f/∂q는 쉽게 구할 수 있음. 하지만 ∂f/∂y를 한 번에 구하기에는 중간의 노드가 있어 조금 불편함
- 이는 chain rule을 통해 구할 수 있음

![image](https://user-images.githubusercontent.com/71377968/164460212-7a4db8d4-ddd4-4d20-aff6-f9325a2a69b9.png)

- chain rule은 구하려는 미분값을 분해하여 구할 수 있다는 것을 의미함
- 우리가 구하고자 하는 global gradient를 local gradients의 곱으로 구할 수 있음
- local gradient는 forward pass 과정에서 구할 수 있음
- 모듈화; 아래의 사진에서처럼, 직접 미분할 수 있는 sigmoid gate를 module로 묶어 직접 미분하여 구현한다면 좀 더 빠르게 backpropagation을 진행할 수 있음

![image](https://user-images.githubusercontent.com/71377968/164460417-56054cb7-e099-4887-88ba-9585c1226f9d.png)

vectorized form: Jacobian matrix

![image](https://user-images.githubusercontent.com/71377968/164460516-520d1c97-c7af-4adb-b412-c94ad24f01ec.png)

- problem: elementwize 함수에서는 Jacobian matrix의 크기가 variable x varaible만큼임
- minibatch를 이용한다면 그의 제곱배만큼 처리를 해야할 수도 있음
- 이 경우, diagonal matrix만 이용하면 됨
- 가능한 이유: 입력의 각 요소는 출력의 해당 요소에만 영향을 미침
- 벡터화된 gradient 계산은 아래와 같이 하면 됨. 단, variable과 결과값의 shape이 같다는 것을 유념해야 함

![image](https://user-images.githubusercontent.com/71377968/164460618-96d20a96-a4cf-4da1-b167-375d79bd40da.png)

### Neural Networks: deep learning에서 기본적으로 사용하는 frame으로, 생물학의 neural network에서 따온 모델

![image](https://user-images.githubusercontent.com/71377968/164460681-f15240b0-ca9b-44fb-aca7-fbd42045a25b.png)

- linear function을 non-linear한 다른 함수에 넣어 layer를 쌓는 방식
- 지난 시간에 배웠던 것으로는 red car에 대해서만 분류가 되었는데, multi layer를 이용하면 yellow/blue car 등 다양한 종류의 자동차가 동일한 자동차 class로 분류될 수 있음
- 각 layer마다 non-linear function이 들어가는데, 이는 이후 강의에서 설명해준다고 함
- 여기서는 max function이 쓰였는데, 이는 ReLU function임
- W1은 input과 직접적으로 연결되어 있고, W2는 h의 score라 생각하면 됨
- layer를 하나씩 더 쌓으려면, 점점 ReLU function을 위에 덧씌우면 됨

인간의 신경망과의 비교

![image](https://user-images.githubusercontent.com/71377968/164460714-5ea253d0-24d3-4359-ba60-65b67e2a7af5.png)

- x0는 인간 신경망에서의 입력 신호와 같음
- 시냅스처럼 입력신호가 cell body와 연결됨
- 인간 신경망의 cell body에서는 들어오는 신호를 종합하는 역할을 하는데, 인공 신경망의 cell body에서는 weighted sum을 구하고 bias term을 더하는 역할을 함
- 생물학적 뉴런은 일정 임계치를 넘으면 다음 뉴런으로 신호를 보내는데, 인공 신경망은 activation function을 통해 활성/비활성을 표시함

![image](https://user-images.githubusercontent.com/71377968/164460767-37fbf342-7d1e-4e40-a4f1-acaaa499794b.png)

주의) biological 뉴런과 인공 뉴런의 메커니즘은 비슷하지만, loose inspiration만 있기에 실제 뉴런은 이보다 복잡함. 이는 수많은 non-linear function으로 이루어지기 때문

![image](https://user-images.githubusercontent.com/71377968/164460807-dc791bde-67f5-4c3a-8be5-888c8513af63.png)

- 2 layer의 경우, input layer에서 입력값이 주어지고, hidden layer에서 Wx가 들어가며, output layer에서는 ReLU function의 결과값이 들어가게 됨
- 3 layer도 동일한 방식

이번 시간에는 인공신경망의 대략적인 구조를 알아보고 gradient를 편하게 계산하기 위한 backpropation, compuational graph에 대해 알아보았다. 다음 시간에는 CNN에 대해 알아볼 것이다.
